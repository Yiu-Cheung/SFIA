# Generated by AI on 2024-12-19
# Reviewed by: AI Assistant
# Ticket: SFIA-001

# Layer: infrastructure
# Responsibility: Implement LLM generation using Ollama
# Public API: OllamaGenerator.generate()

from typing import List, Dict, Any

from haystack_integrations.components.generators.ollama import OllamaGenerator as HaystackOllamaGenerator

from ...domain.models.document import Document
from ...domain.models.query import Query
from ...domain.models.response import Response


class OllamaGenerator:
    """Ollama-based LLM generator implementation."""
    
    def __init__(
        self,
        model: str = "llama3.2:latest",
        url: str = "http://localhost:11434",
        generation_kwargs: Dict[str, Any] = None
    ) -> None:
        """Initialize the Ollama generator."""
        if generation_kwargs is None:
            generation_kwargs = {
                "num_predict": 5120,
                "temperature": 0.9,
            }
        
        self._generator = HaystackOllamaGenerator(
            model=model,
            url=url,
            generation_kwargs=generation_kwargs
        )
    
    def generate(
        self, 
        query: Query, 
        documents: List[Document]
    ) -> Response:
        """Generate a response based on query and retrieved documents."""
        # Build context from documents
        context = "\n".join([doc.content for doc in documents])
        
        # Create prompt
        prompt = self._build_prompt(query.text, context)
        
        # Generate response
        result = self._generator.run(prompt=prompt)
        replies = result.get("replies", [])
        
        if not replies:
            raise RuntimeError("No response generated from Ollama")
        
        return Response(
            content=replies[0],
            metadata={"model": "ollama", "replies_count": len(replies)}
        )
    
    def _build_prompt(self, query: str, context: str) -> str:
        """Build the prompt for the LLM."""
        return f"""Answer the following question as naturally and conversationally as possible, using the information below as your main source. 
If the answer is not found, use your knowledge.

Context:
{context}

Question: {query}?""" 