# Generated by AI on 2024-12-19
# Reviewed by: AI Assistant
# Ticket: SFIA-002

# Layer: application
# Responsibility: Orchestrate document loading and storage operations with optimized XLSX processing
# Public API: DocumentService.load_from_folder(), DocumentService.get_documents()

import os
from typing import List

import pandas as pd

from ...domain.models.document import Document
from ...domain.models.query import Query
from ...domain.repositories.document_repository import DocumentRepository
from ...infrastructure.persistence.xlsx_document_converter import XLSXDocumentConverter
from ...infrastructure.persistence.structured_xlsx_converter import StructuredXLSXConverter


class DocumentService:
    """Application service for document operations."""
    
    def __init__(self, document_repository: DocumentRepository) -> None:
        """Initialize the document service."""
        self._document_repository = document_repository
        # Use structured converter for better AI comprehension
        self._xlsx_converter = StructuredXLSXConverter(
            save_debug_files=True,
            debug_folder="doc/.converted",
            output_format="json"
        )
    
    def load_from_folder(self, folder_path: str) -> List[Document]:
        """Load documents from a folder containing txt and Excel files."""
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"Folder not found: {folder_path}")
        
        documents = []
        
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            
            if filename.endswith('.txt'):
                documents.extend(self._load_txt_file(file_path))
            elif filename.endswith(('.xlsx', '.xls')):
                documents.extend(self._load_excel_file(file_path))
        
        if documents:
            self._document_repository.save_all(documents)
        
        return documents
    
    def _load_txt_file(self, file_path: str) -> List[Document]:
        """Load content from a text file."""
        documents = []
        try:
            with open(file_path, encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    documents.append(Document(
                        content=content,
                        metadata={"source": file_path, "type": "txt"}
                    ))
        except Exception as e:
            print(f"Failed to read {file_path}: {e}")
        
        return documents
    
    def _load_excel_file(self, file_path: str) -> List[Document]:
        """Load content from an Excel file using optimized Haystack converter."""
        try:
            # Use the optimized XLSX converter for better AI reading
            documents = self._xlsx_converter.convert_file(file_path)
            print(f"Successfully converted {file_path} using Haystack XLSX converter")
            return documents
        except Exception as e:
            print(f"Failed to convert {file_path} with Haystack converter: {e}")
            # Fallback to original pandas-based method
            return self._load_excel_file_fallback(file_path)
    
    def _load_excel_file_fallback(self, file_path: str) -> List[Document]:
        """Fallback method to load content from an Excel file using pandas."""
        documents = []
        try:
            # Read all sheets from the Excel file
            excel_file = pd.ExcelFile(file_path)
            
            for sheet_name in excel_file.sheet_names:
                # Skip sheets that are not relevant for SFIA skills
                if sheet_name.lower() in ['read me notes', 'terms of use']:
                    continue
                
                df = pd.read_excel(
                    file_path, 
                    sheet_name=sheet_name,
                    engine='openpyxl' if file_path.endswith('.xlsx') else None
                )
                
                # For Skills sheet, create individual documents for each skill
                if sheet_name.lower() == 'skills':
                    documents.extend(self._process_skills_sheet(df, file_path, sheet_name))
                else:
                    # For other sheets, combine all content
                    all_content = []
                    for column in df.columns:
                        for value in df[column]:
                            content = str(value).strip()
                            if content and content.lower() != 'nan':
                                all_content.append(content)
                    
                    if all_content:
                        combined_content = "\n".join(all_content)
                        documents.append(Document(
                            content=combined_content,
                            metadata={"source": file_path, "type": "excel", "sheet": sheet_name, "rows_processed": len(df)}
                        ))
        except Exception as e:
            print(f"Failed to read {file_path}: {e}")
        
        return documents

    def _process_skills_sheet(self, df: pd.DataFrame, file_path: str, sheet_name: str) -> List[Document]:
        """Process the Skills sheet to create individual documents for each skill."""
        documents = []
        
        # Look for the Skill column
        skill_column = None
        for col in df.columns:
            if 'skill' in str(col).lower():
                skill_column = col
                break
        
        if skill_column is None:
            # If no skill column found, process as regular sheet
            all_content = []
            for column in df.columns:
                for value in df[column]:
                    content = str(value).strip()
                    if content and content.lower() != 'nan':
                        all_content.append(content)
            
            if all_content:
                combined_content = "\n".join(all_content)
                documents.append(Document(
                    content=combined_content,
                    metadata={"source": file_path, "type": "excel", "sheet": sheet_name, "rows_processed": len(df)}
                ))
            return documents
        
        # Process each skill row
        for index, row in df.iterrows():
            skill_name = str(row[skill_column]).strip()
            if skill_name and skill_name.lower() != 'nan' and skill_name != 'Skill':
                # Create content for this skill
                skill_content = f"Skill: {skill_name}\n"
                
                # Add level descriptions if available
                for col in df.columns:
                    if 'level' in str(col).lower() and 'description' in str(col).lower():
                        level_desc = str(row[col]).strip()
                        if level_desc and level_desc.lower() != 'nan':
                            skill_content += f"{col}: {level_desc}\n"
                
                # Add overall description if available
                for col in df.columns:
                    if 'overall description' in str(col).lower():
                        overall_desc = str(row[col]).strip()
                        if overall_desc and overall_desc.lower() != 'nan':
                            skill_content += f"Overall description: {overall_desc}\n"
                
                # Add other relevant columns
                for col in df.columns:
                    if col not in [skill_column] and 'level' not in str(col).lower():
                        value = str(row[col]).strip()
                        if value and value.lower() != 'nan':
                            skill_content += f"{col}: {value}\n"
                
                documents.append(Document(
                    content=skill_content,
                    metadata={"source": file_path, "type": "excel", "sheet": sheet_name, "skill": skill_name, "row": index}
                ))
        
        return documents
    
    def get_documents(self) -> List[Document]:
        """Get all documents from the repository."""
        # This is a simplified implementation - in a real scenario,
        # you might want to implement pagination or filtering
        return self._document_repository.search(
            Query("all"), 
            top_k=1000  # Large number to get all documents
        ) 